ğ—–ğ—¹ğ—®ğ˜‚ğ—±ğ—² ğ—–ğ—¼ğ—±ğ—² ğ—¼ğ—» ğ˜€ğ˜ğ—²ğ—¿ğ—¼ğ—¶ğ—±ğ˜€ - ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ ğ—¦ğ—½ğ—²ğ—°-ğ——ğ—¿ğ—¶ğ˜ƒğ—²ğ—» ğ——ğ—²ğ˜ƒğ—²ğ—¹ğ—¼ğ—½ğ—ºğ—²ğ—»ğ˜ (ğ—¦ğ——ğ——) ğ—¶ğ—»ğ˜€ğ˜ğ—²ğ—®ğ—± ğ—¼ğ—³ â€œğ˜ƒğ—¶ğ—¯ğ—² ğ—°ğ—¼ğ—±ğ—¶ğ—»ğ—´.â€

That's what GSD (Get Shit Done) feels like after using it for the last two weeks. Didn't expect much from another Claude Code repo, after testing hundreds. GSD proved me wrong. Iâ€™m genuinely impressed.

ğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ—šğ—¦ğ——?
Repo that adds a context engineering layer that turns AI-coding into actual shipping. Claude Code is brilliant - until context rot kicks in. 50k tokens in, it's "being more concise." 100k in, it's hallucinating functions. 150k in, you're debugging its debugging. GSD's brutal fix: every task gets a fresh 200k context window. No accumulated garbage and no degradation.

ğ—§ğ—µğ—² ğ˜„ğ—¼ğ—¿ğ—¸ğ—³ğ—¹ğ—¼ğ˜„ ğ—¶ğ˜ ğ—³ğ—¼ğ—¹ğ—¹ğ—¼ğ˜„ğ˜€:
âœ¦ Discuss â†’ AI asks until it truly gets your vision
âœ¦ Research â†’ Parallel agents investigate the domain
âœ¦ Plan â†’ Atomic tasks with built-in verification
âœ¦ Execute â†’ Fresh context per task, one surgical commit each
âœ¦ Verify â†’ You confirm it works, AI auto-generates fix plans if not

It feels like magic, but it's just scaffolding designed for how LLMs actually behave - not how we wish they did.

Install with one command: npx get-shit-done-cc
Repo: https://lnkd.in/eMPEJGQv

Anyone here experimenting with this? Whatâ€™s your take so far?
